Sfida 1: Etica
--------------

Il tema dell’Intelligenza Artificiale (IA), come la comparsa e
l’affermazione di ogni nuova tecnologia, ripropone la contrapposizione
tra “apocalittici e integrati” [1]_. Gli **apocalittici** temono che
l'Intelligenza Artificiale prenderà il sopravvento sulle persone,
deciderà per loro, ruberà loro il lavoro, le discriminerà, ne violerà la
*privacy*, e le controllerà di nascosto condizionandone la vita. Gli
**integrati** sognano invece un mondo dove le macchine siano capaci di
condurre autonomamente processi burocratici, di essere impiegate come
potenti strumenti di calcolo per elaborare e interpretare nella maniera
migliore grandi quantità di dati, sostituendo gli uomini nei compiti più
gravosi e ripetitivi, e di creare soluzioni in grado di diminuire
crimini e debellare malattie.

In sostanza esistono due percezioni della tecnologia, spesso in
conflitto tra loro. Quella degli “apocalittici” valuta negativamente
l’inserimento dell’IA nella Pubblica amministrazione (PA) adducendo come
motivazione una serie di criticità che rischierebbero di avere effetti
negativi non solo su efficienza ed efficacia dei provvedimenti ma anche
sui diritti dei cittadini. Quella degli “integrati”, invece, giudica
l’utilizzo dell’IA come estremamente positivo, ritiene che
l’implementazione di queste tecnologie possano migliorare
significativamente non solo l’azione della PA ma anche la qualità di
vita dei cittadini e che sia quindi necessaria un totale ed
incondizionato processo di ricerca e sviluppo in tale ambito [2]_. Due
punti di vista estremi, ognuno con differenti peculiarità, che vanno
analizzati criticamente per poter risolvere i punti deboli segnalati
dagli “apocalittici” e regolare i punti di forza sostenuti dagli
“integrati”.

Gli esempi sopra richiamati non sono scelti a caso, ma sono il frutto
del dibattito che in questi anni si è sollevato nella comunità
scientifica e nella società civile rispetto all’impatto dei sistemi di
IA sulle nostre vite.

La sfida etica dell’introduzione di soluzioni di Intelligenza
Artificiale è rappresentata dall’esigenza di rispondere in maniera
equilibrata alla polarizzazione di queste due visioni, integrando
l’innovazione e tenendo conto degli effetti che questa ha già avuto e
continuerà ad avere nello sviluppo della società, rispettando e
salvaguardando i valori fondamentali universalmente riconosciuti.

La Pubblica amministrazione è chiamata, quindi, ad affrontare questioni
etiche numerose e complesse. Per comprenderne la portata è possibile
analizzare quelli che rappresentano gli elementi centrali nel dibattito
pubblico e nell’analisi scientifica:

-  **qualità e neutralità dei dati**: i sistemi di apprendimento
   automatico hanno bisogno di dati “annotati” da esseri umani
   (*supervised learning*) o quantomeno selezionati e preparati
   (*unsupervised learning*). Assimilano con questo anche gli errori o i
   pregiudizi (*bias*) introdotti anche involontariamente dai
   progettisti, replicandoli in ogni futura applicazione. Ad esempio,
   *dataset* con *bias* propagano gli stessi errori di valutazione nel
   significato di un'immagine o di un concetto, come è avvenuto, per
   esempio, con alcuni algoritmi utilizzati per prevenire i crimini,
   dove i dati erano viziati da una serie storica che enfatizzava
   differenze etniche [3]_. Oppure *dataset* sbilanciati, che
   sovrastimano o sottostimano il peso di alcune variabili nella
   ricostruzione della relazione causa–effetto necessaria per spiegare
   certi eventi e, soprattutto, per prevederli;
   
-  **responsabilità (*accountability* e *liability)***: gli esempi
   appena riportati mettono in evidenza il forte impatto che ha
   l’Intelligenza Artificiale sull'attività decisionale dei soggetti
   pubblici. Sia nel caso che questa agisca come assistente degli esseri
   umani sia come soggetto autonomo, l’IA genera degli effetti sulla
   vita delle persone in relazione ai quali è necessario poter
   configurare una responsabilità giuridica. Tuttavia la titolarità di
   quest’ultima risulta ad oggi non ancora chiaramente definita, dato
   che potrebbe essere attribuita al produttore [4]_ o al
   possessore [5]_ dell'Intelligenza Artificiale, oppure ancora al suo
   utente finale [6]_. Chi progetta sistemi di IA può essere
   responsabile di difetti di disegno o implementazione, ma non di
   comportamenti causati da *dataset* di addestramento inadeguati. Un
   decisore pubblico può essere ritenuto politicamente responsabile
   delle decisioni prese sulla base di algoritmi che processano dati
   affetti dai *bias* citati sopra? Quale tipo di responsabilità può
   essere configurata in capo allo Pubblica amministrazione? Se un robot
   fa del male a qualcuno, chi deve essere ritenuto responsabile e chi,
   eventualmente, ha l’obbligo di risarcire la vittima (e con quale
   patrimonio)? Il decisore pubblico può trasferire la propria
   responsabilità politica ad un sistema di IA che non risponde ad un
   chiaro principio di rappresentanza? È eticamente sostenibile che, al
   fine di migliorare quindi l’efficienza e l’efficacia dei
   provvedimenti, alcune scelte importanti possano essere prese con
   l’influenza di una IA? E nel concedere fiducia ad un sistema IA, come
   controllarne la coerenza nel tempo? Queste sono solo alcune delle
   questioni che emergono in questo ambito e che evidenziano la
   necessità di stabilire dei principi per l’utilizzo delle tecnologie
   di IA in un contesto pubblico.

-  **trasparenza e apertura**: il tema della responsabilità della
   Pubblica amministrazione ha a che vedere anche con i doveri a cui
   quest'ultima deve ottemperare nei confronti dei cittadini, nel
   momento in cui stabilisce di fornire loro dei servizi o di prendere
   decisioni che li riguardano, servendosi di soluzioni di Intelligenza
   Artificiale. Il funzionamento di questi ultimi deve rispondere a
   criteri di trasparenza e apertura. La trasparenza si trasforma in un
   prerequisito fondamentale per evitare discriminazioni e risolvere il
   problema dell'asimmetria informativa, garantendo al cittadino il
   diritto alla comprensione delle decisioni pubbliche. È necessario
   ragionare anche su *policy* di *benchmark* per evitare effetti di
   dimensione più vasta: così come un amministratore può agire in modo
   poco trasparente perseguendo non il bene comune ma interessi privati,
   un algoritmo non trasparente potrebbe realizzare gli stessi illeciti
   in modo persino più ampio, producendo non solo ingiustizie ma anche
   discriminazioni sociali.

-  **tutela della sfera privata**: ulteriore esigenza, strettamente
   legata alla precedente, è quella di tutelare i dati degli individui.
   La PA dovrà progettare servizi basati sull’IA in grado di garantire
   efficienza e tempestività di risposta ma anche protezione dei dati
   sensibili dei cittadini. Tale requisito, strettamente connesso al
   contesto legale, presenta alcune peculiarità etiche che riguardano
   l’utilizzo che la PA può fare dei dati a sua conoscenza in contesti
   differenti da quelli in cui sono stati raccolti. È eticamente
   sostenibile che la PA, mediante l’utilizzo di dati raccolti per altri
   scopi, prenda provvedimenti sulla base delle nuove informazioni
   derivate? È lecito utilizzare questi dati per alimentare sistemi
   predittivi?

Per affrontare queste sfide, può essere utile seguire alcuni principi
generali. Tra questi possiamo menzionare la necessità di un approccio
*antropocentrico*\  [7]_ secondo cui l'Intelligenza Artificiale deve
essere sempre messa al servizio delle persone e non viceversa [8]_. Ci
sono, inoltre, principi di equità, come quello procedurale (non
arbitrarietà delle procedure), formale (uguale trattamento per individui
o gruppi uguali) e sostanziale (rimozione effettiva degli ostacoli di
natura economico-sociale), così come il soddisfacimento di alcuni
bisogni di base universali tra cui il rispetto delle libertà e dei
diritti degli individui e della collettività [9]_.

.. [1]
   Umberto Eco, *Apocalittici e integrati*, ed. Bompiani, 1964.

.. [2]
   Alle utopie della “ideologia californiana” (Richard Barbrook,
   *Imaginary Futures: From Thinking Machines to the Global Village,*
   2007) si contrappongono oggi critiche radicali al “soluzionismo”
   tecnologico (Eugenij Morozov, *To Save Everything, Click Here. The
   Folly of Technological Solutionism,* 2013)

.. [3]
   Bruno Lepri, Nuria Oliver, Emmanuel Letouz, Alex Pentland, Patrick
   Vinck, "Fair, transparent and accountable algorithmic decision-making
   processes. The premise, the proposed solutions, and the open
   challenges", Science business media, Springer, 2017

.. [4]
   Ci sono reti neurali i cui algoritmi di calcolo non sono del tutto
   ricostruibili nemmeno dai loro programmatori, generando quello che
   viene definito “effetto *black-box*\ ”. Si veda, su questi temi:
   `https://arxiv.org/pdf/1706.08606.pdf <https://arxiv.org/pdf/1706.08606.pdf>`__,
   `https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/ <https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/>`__

.. [5]
   Come avviene attualmente nel campo della robotica.

.. [6]
   Con un parallelo, potremmo portare il caso delle opere di
   costruzione. Il costruttore ne ha piena responsabilità per i primi
   anni dall’inaugurazione dell’opera, ma poi la responsabilità passa al
   responsabile della manutenzione della stessa.

.. [7]
   Cfr.
   `http://www.g7italy.it/sites/default/files/documents/ANNEX2-Artificial_Intelligence_0.pdf <http://www.g7italy.it/sites/default/files/documents/ANNEX2-Artificial_Intelligence_0.pdf>`__

.. [8]
   Necessario, parafrasando il pensiero Kantiano, che l’IA “tratti
   l’uomo sempre come fine e mai come uno dei mezzi”. Immanuel Kant,
   *Fondazione della metafisica dei costumi,* 1785. Per esempio, le
   famose leggi della robotica di Asimov vanno in questa direzione: un
   robot non può recar danno a un essere umano né può permettere che, a
   causa del proprio mancato intervento, un essere umano riceva danno;
   un robot deve obbedire agli ordini impartiti dagli esseri umani,
   purché tali ordini non contravvengano alla Prima Legge; un robot deve
   proteggere la propria esistenza, purché questa autodifesa non
   contrasti con la Prima o con la Seconda Legge.

.. [9]
   Sulla scorta di concetti molto simili, alcuni Stati, come per esempio
   il Canada
   (`https://medium.com/code-for-canada/responsible-ai-in-the-government-of-canada-a-sneak-peek-973727477bdf <https://medium.com/code-for-canada/responsible-ai-in-the-government-of-canada-a-sneak-peek-973727477bdf>`__),
   hanno provato a istituire una sorta di decalogo, capace di guidare
   tutte le scelte della loro Pubblica amministrazione nell'ambito
   dell'Intelligenza Artificiale. C'è però anche chi ritiene che non si
   possano applicare dei principi generali di etica a tutti i settori in
   cui l'Intelligenza Artificiale può operare, ma sarebbe meglio
   organizzare delle consultazioni settoriali, guidate dalle istituzioni
   ma aperte anche agli stakeholder, al fine di capire quali sono i
   codici e le carte etiche da applicare alle varie sfere della vita
   civile.
